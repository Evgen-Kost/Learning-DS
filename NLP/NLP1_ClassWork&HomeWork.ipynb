{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIvhwE4zxX0s",
    "outputId": "dfa3b3dd-38a1-4a67-879f-46376640ef14"
   },
   "outputs": [],
   "source": [
    "#Установка нужных пакетов\n",
    "#!pip install --upgrade nltk gensim bokeh umap-learn\n",
    "\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hF9WPCtfxZR9",
    "outputId": "0d73ae54-58c4-4d44-bcc7-8d35c800f08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘./quora.txt’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "# выгружаем датасет:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MaFpN9pvxtNg",
    "outputId": "f8255b0e-bdfc-47cf-fa91-7f9075b78d14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvXRbOKGx0l_",
    "outputId": "f265a2aa-fc44-4763-bc87-fe05ee0a7203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovkxi_QOySCl"
   },
   "source": [
    "#Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EK7uvHi6zeWY"
   },
   "outputs": [],
   "source": [
    "data_tok = []# YOUR CODE HERE\n",
    "\n",
    "for i in range(len(data)):\n",
    "    lowerWord = data[i].lower()\n",
    "    data_tok.append(tokenizer.tokenize(lowerWord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EK7uvHi6zeWY"
   },
   "outputs": [],
   "source": [
    "data_tok = []# YOUR CODE HERE\n",
    "\n",
    "for i in range(len(data)):\n",
    "    lowerWord = data[i].lower()\n",
    "    data_tok.append(tokenizer.tokenize(lowerWord))\n",
    "\n",
    "#checking\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EK7uvHi6zeWY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'are',\n",
       " 'some',\n",
       " 'ways',\n",
       " 'to',\n",
       " 'overcome',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'food',\n",
       " 'addiction',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tok[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtKeoLCYzY4j"
   },
   "source": [
    "###Задание 2: Подсчитайте топ10 самых популярных лем в рамках data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a_BxzSv9yR0w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/mors/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mors/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data_tok_lemma = []\n",
    "\n",
    "for i in range(len(data_tok)):\n",
    "    for j in range(len(data_tok[i])):\n",
    "        data_tok_lemma.append(lemmatizer.lemmatize(data_tok[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a_BxzSv9yR0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('?', 552413), ('the', 252068), ('what', 214798), ('is', 185392), ('a', 172513), ('i', 149873), ('to', 141788), ('in', 139813), ('how', 135687), ('of', 112001)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(data_tok_lemma).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1SM3sn1zf1b"
   },
   "source": [
    "###Задание 3: Подсчитайте количество разных слов до и после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q88BIteDzpWR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mors/anaconda3/lib/python3.9/site-packages/numpy/lib/arraysetops.py:270: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536935\n",
      "80303\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "data_tok_unique = numpy.unique(data_tok)\n",
    "data_tok_lemma_unique = numpy.unique(data_tok_lemma)\n",
    "\n",
    "print(len(data_tok_unique))\n",
    "print(len(data_tok_lemma_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxKa8yUUzqNN"
   },
   "source": [
    "###Задание 4: Подсчитайте количество разных слов до и после стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x91DX51qzszR"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "data_tok_stem = []# YOUR CODE HERE\n",
    "\n",
    "for i in range(len(data_tok)):\n",
    "    for j in range(len(data_tok[i])):\n",
    "        data_tok_stem.append(stemmer.stem(data_tok[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "x91DX51qzszR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536935\n",
      "67026\n"
     ]
    }
   ],
   "source": [
    "data_tok_stem_unique = numpy.unique(data_tok_stem)\n",
    "\n",
    "print(len(data_tok_unique))\n",
    "print(len(data_tok_stem_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXA7Fe_izuqh"
   },
   "source": [
    "###Задание 5: Подсчитайте количество разных слов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BGgmHzUAzwqO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537251\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "data_unique = numpy.unique(data)\n",
    "\n",
    "print(len(data_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9iloRCVShn"
   },
   "source": [
    "REGEXP\n",
    "\n",
    "https://www.programiz.com/python-programming/regex \n",
    "\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "521aLisyVUg_",
    "outputId": "fd21b30e-aadf-47eb-e776-ba59ff427d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search unsuccessful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a*s'\n",
    "test_string = 'abyss'\n",
    "result = re.match(pattern, test_string)\n",
    "\n",
    "if result:\n",
    "  print(\"Search successful.\")\n",
    "else:\n",
    "  print(\"Search unsuccessful.\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###Задание 6: \n",
    "https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true \n",
    "\n",
    "###Задание 7: \n",
    "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "###Задание 8: \n",
    "https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "###Задание 9: \n",
    "https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "521aLisyVUg_",
    "outputId": "fd21b30e-aadf-47eb-e776-ba59ff427d7f"
   },
   "outputs": [],
   "source": [
    "###Задание 6 https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true \n",
    "\n",
    "Regex_Pattern = r'hackerrank'\t# Do not delete 'r'.\n",
    "\n",
    "###Задание 7 https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = r\"\\S{2}\\s{1}\\S{2}\\s{1}\\S{2}\"\n",
    "\n",
    "## На занятии успел досюда :) \n",
    "\n",
    "##Задание 8: https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = r\"^\\d{1}\\w{4}.{1}$\"\t\n",
    "\n",
    "## Задание 9: https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = r'\\b[aeiouAEIOU]{1}[a-zA-Z]*\\b'\n",
    "\n",
    "## Redex на занятии\n",
    "\n",
    "Regex_Pattern = '\\w{2}\\d+[0-1]{1}.{1}$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csv2YN2IRXJB"
   },
   "source": [
    "Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8YWG3JhSFeZ",
    "outputId": "325b4fa5-7d56-4993-b4d0-1aae78bad94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZ1qx05Q46b"
   },
   "source": [
    "Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Tew2nQN4OCiW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7131345\n",
      "87819\n",
      "87811\n",
      "['can', 'i', 'get', 'back', 'with', 'my', 'ex', 'even', 'though', 'she', 'is', 'pregnant', 'with', 'another', 'guy', \"'\", 's', 'baby', '?']\n",
      "[1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "['what', 'are', 'some', 'ways', 'to', 'overcome', 'a', 'fast', 'food', 'addiction', '?']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "data_tok_all = []\n",
    "\n",
    "for i in range(len(data_tok)):\n",
    "    for j in range(len(data_tok[i])):\n",
    "        data_tok_all.append(data_tok[i][j])\n",
    "\n",
    "print(len(data_tok_all))\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(data_tok_all)\n",
    "print(len(vocab))\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "    \n",
    "print(len(filtered_vocab))\n",
    "\n",
    "vector1=vectorize(data_tok[0])\n",
    "print(data_tok[0])\n",
    "print(vector1[0:9])\n",
    "vector2=vectorize(data_tok[1])\n",
    "print(data_tok[1])\n",
    "print(vector2[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HomeWork ( https://regex101.com/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###ML1_1: \n",
    "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
    "\n",
    "###ML1_2: \n",
    "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
    "\n",
    "###ML1_3: \n",
    "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
    "\n",
    "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ML1_1: https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = r'(?:ok){3,}'\n",
    "\n",
    "###ML1_2: https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = '(^\\d{2}[:]{1}\\d{2}[:]{1}\\d{2}[:]{1}\\d{2}$)|(^\\d{2}[.]{1}\\d{2}[.]{1}\\d{2}[.]{1}\\d{2}$)|(^\\d{2}[,]{1}\\d{2}[,]{1}\\d{2}[,]{1}\\d{2}$)|(^\\d{2}[-]{1}\\d{2}[-]{1}\\d{2}[-]{1}\\d{2}$)|(^\\d{2}[-]{3}\\d{2}[-]{3}\\d{2}[-]{3}\\d{2}$)'\n",
    "\n",
    "###ML1_3: https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
    "\n",
    "Regex_Pattern = r'<a href=\"(.*?)\".*?>([\\w ,./]*)(?=</)'\n",
    "\n",
    "import sys, re\n",
    "\n",
    "input()\n",
    "html = sys.stdin.read()\n",
    "tags =  set(re.findall(\"<\\s*([a-z0-9]+)\", html))\n",
    "tags = list(tags)\n",
    "tags.sort()\n",
    "print(\";\".join(tags))\n",
    "\n",
    "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     Верблюдов-то за что? Дебилы, бл...\\n\n",
      "1        Хохлы, это отдушина затюканого россиянина, мол...\n",
      "2                                Собаке - собачья смерть\\n\n",
      "3        Страницу обнови, дебил. Это тоже не оскорблени...\n",
      "4        тебя не убедил 6-страничный пдф в том, что Скр...\n",
      "                               ...                        \n",
      "14407    Вонючий совковый скот прибежал и ноет. А вот и...\n",
      "14408    А кого любить? Гоблина тупорылого что-ли? Или ...\n",
      "14409    Посмотрел Утомленных солнцем 2. И оказалось, ч...\n",
      "14410    КРЫМОТРЕД НАРУШАЕТ ПРАВИЛА РАЗДЕЛА Т.К В НЕМ Н...\n",
      "14411    До сих пор пересматриваю его видео. Орамбо кст...\n",
      "Name: comment, Length: 14412, dtype: object\n"
     ]
    }
   ],
   "source": [
    "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('labeled.csv')\n",
    "\n",
    "print(df['comment']) \n",
    "\n",
    "data = df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['верблюдов', '-', 'то', 'за', 'что', '?', 'дебилы', ',', 'бл', '...']\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "data_tok = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    lowerWord = data[i].lower()\n",
    "    data_tok.append(tokenizer.tokenize(lowerWord))\n",
    "\n",
    "print(data_tok[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['верблюд', '-', 'то', 'за', 'что', '?', 'дебил', ',', 'бл', '...']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\") \n",
    "\n",
    "data_stem = []\n",
    "\n",
    "for i in range(len(data_tok)):\n",
    "    for j in range(len(data_tok[i])):\n",
    "        data_stem.append(stemmer.stem(data_tok[i][j]))\n",
    "\n",
    "print(data_stem[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['верблюдов', '-', 'то', 'за', 'что', '?', 'дебилы', ',', 'бл', '...']\n",
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "['хохлы', ',', 'это', 'отдушина', 'затюканого', 'россиянина', ',', 'мол', ',', 'вон', ',', 'а', 'у', 'хохлов', 'еще', 'хуже', '.', 'если', 'бы', 'хохлов', 'не', 'было', ',', 'кисель', 'их', 'бы', 'придумал', '.']\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "data_tok_all = []\n",
    "\n",
    "for i in range(len(data_tok)):\n",
    "    for j in range(len(data_tok[i])):\n",
    "        data_tok_all.append(data_tok[i][j])\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(data_tok_all)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in russian_stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "\n",
    "vector1=vectorize(data_tok[0])\n",
    "print(data_tok[0])\n",
    "print(vector1[:10])\n",
    "vector2=vectorize(data_tok[1])\n",
    "print(data_tok[1])\n",
    "print(vector2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Зачем ты пишешь хуйню, дегенерат? Поцелуй в губы ! поцелую в засос.\n",
      "\n",
      "писать хуйня дегенерат поцелуй губа поцеловать засос\n"
     ]
    }
   ],
   "source": [
    "# Lemmas\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "mystem = Mystem() \n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(data[10])\n",
    "print(preprocess_text(data[10]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9664aef0baf30e2b509990f72aa3245ea02a06f685dd1680e2e8f9c3deed265"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
